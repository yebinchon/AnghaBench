#define NULL ((void*)0)
typedef unsigned long size_t;  // Customize by platform.
typedef long intptr_t; typedef unsigned long uintptr_t;
typedef long scalar_t__;  // Either arithmetic or pointer type.
/* By default, we understand bool (as a convenience). */
typedef int bool;
#define false 0
#define true 1

/* Forward declarations */
typedef  struct TYPE_8__   TYPE_4__ ;
typedef  struct TYPE_7__   TYPE_3__ ;
typedef  struct TYPE_6__   TYPE_2__ ;
typedef  struct TYPE_5__   TYPE_1__ ;

/* Type definitions */
typedef  int uint16_t ;
typedef  int u8 ;
typedef  int /*<<< orphan*/  u64 ;
typedef  scalar_t__ u32 ;
struct TYPE_8__ {scalar_t__ msn; size_t cidx; TYPE_3__* sw_rq; } ;
struct TYPE_6__ {size_t cidx; int size; struct t4_swsqe* sw_sq; int /*<<< orphan*/  in_use; TYPE_1__* oldest_read; } ;
struct t4_wq {TYPE_4__ rq; TYPE_2__ sq; scalar_t__ flushed; } ;
struct t4_cqe {int /*<<< orphan*/  header; } ;
struct t4_swsqe {int complete; int /*<<< orphan*/  wr_id; struct t4_cqe cqe; } ;
struct t4_srq {int dummy; } ;
struct t4_cq {int /*<<< orphan*/  cidx; int /*<<< orphan*/  cqid; int /*<<< orphan*/  sw_cidx; } ;
struct TYPE_7__ {int /*<<< orphan*/  wr_id; } ;
struct TYPE_5__ {int /*<<< orphan*/  signaled; } ;

/* Variables and functions */
 int /*<<< orphan*/  FUNC0 (struct t4_cqe*) ; 
 int /*<<< orphan*/  FUNC1 (struct t4_cqe*) ; 
 int /*<<< orphan*/  FUNC2 (struct t4_cqe*) ; 
 scalar_t__ FUNC3 (struct t4_cqe*) ; 
 int /*<<< orphan*/  FUNC4 (struct t4_cqe*) ; 
 int /*<<< orphan*/  FUNC5 (struct t4_cqe*) ; 
 scalar_t__ FUNC6 (struct t4_cqe*) ; 
 int /*<<< orphan*/  FUNC7 (int /*<<< orphan*/ ) ; 
 int FUNC8 (struct t4_cqe*) ; 
 int /*<<< orphan*/  FUNC9 (struct t4_cqe*) ; 
 int /*<<< orphan*/  FUNC10 (struct t4_cqe*) ; 
 scalar_t__ FUNC11 (struct t4_cqe*) ; 
 size_t FUNC12 (struct t4_cqe*) ; 
 int FUNC13 (struct t4_cqe*) ; 
 scalar_t__ FUNC14 (struct t4_cqe*) ; 
 int EAGAIN ; 
 scalar_t__ FW_RI_READ_RESP ; 
 scalar_t__ FW_RI_TERMINATE ; 
 scalar_t__ FUNC15 (struct t4_cqe*) ; 
 scalar_t__ FUNC16 (struct t4_cqe*) ; 
 scalar_t__ FUNC17 (struct t4_cqe*) ; 
 int /*<<< orphan*/  T4_ERR_MSN ; 
 scalar_t__ T4_ERR_SWFLUSH ; 
 int /*<<< orphan*/  FUNC18 (struct t4_wq*) ; 
 int /*<<< orphan*/  FUNC19 (struct t4_wq*,struct t4_cqe*) ; 
 scalar_t__ c4iw_wr_log ; 
 int /*<<< orphan*/  FUNC20 (int /*<<< orphan*/ ) ; 
 int /*<<< orphan*/  FUNC21 (struct t4_wq*,struct t4_cqe*,struct t4_cqe*) ; 
 int /*<<< orphan*/  FUNC22 (struct t4_wq*,struct t4_cq*) ; 
 int /*<<< orphan*/  FUNC23 (char*,...) ; 
 int /*<<< orphan*/  FUNC24 (struct t4_cqe*,struct t4_srq*) ; 
 int /*<<< orphan*/  FUNC25 (struct t4_cq*) ; 
 int FUNC26 (struct t4_cq*,struct t4_cqe**) ; 
 int /*<<< orphan*/  FUNC27 (struct t4_wq*) ; 
 int /*<<< orphan*/  FUNC28 (struct t4_wq*,int /*<<< orphan*/ ) ; 
 int /*<<< orphan*/  FUNC29 (struct t4_wq*) ; 
 int /*<<< orphan*/  FUNC30 (struct t4_cq*) ; 
 scalar_t__ FUNC31 (struct t4_wq*) ; 
 scalar_t__ FUNC32 (int) ; 

__attribute__((used)) static int FUNC33(struct t4_wq *wq, struct t4_cq *cq, struct t4_cqe *cqe,
		   u8 *cqe_flushed, u64 *cookie, u32 *credit,
		   struct t4_srq *srq)
{
	int ret = 0;
	struct t4_cqe *hw_cqe, read_cqe;

	*cqe_flushed = 0;
	*credit = 0;
	ret = FUNC26(cq, &hw_cqe);
	if (ret)
		return ret;

	FUNC23("CQE OVF %u qpid 0x%0x genbit %u type %u status 0x%0x opcode 0x%0x len 0x%0x wrid_hi_stag 0x%x wrid_low_msn 0x%x\n",
		 FUNC4(hw_cqe), FUNC5(hw_cqe),
		 FUNC1(hw_cqe), FUNC8(hw_cqe), FUNC6(hw_cqe),
		 FUNC3(hw_cqe), FUNC2(hw_cqe), FUNC9(hw_cqe),
		 FUNC10(hw_cqe));

	/*
	 * skip cqe's not affiliated with a QP.
	 */
	if (wq == NULL) {
		ret = -EAGAIN;
		goto skip_cqe;
	}

	/*
	* skip hw cqe's if the wq is flushed.
	*/
	if (wq->flushed && !FUNC17(hw_cqe)) {
		ret = -EAGAIN;
		goto skip_cqe;
	}

	/*
	 * skip TERMINATE cqes...
	 */
	if (FUNC3(hw_cqe) == FW_RI_TERMINATE) {
		ret = -EAGAIN;
		goto skip_cqe;
	}

	/*
	 * Special cqe for drain WR completions...
	 */
	if (FUNC14(hw_cqe)) {
		*cookie = FUNC0(hw_cqe);
		*cqe = *hw_cqe;
		goto skip_cqe;
	}

	/*
	 * Gotta tweak READ completions:
	 *	1) the cqe doesn't contain the sq_wptr from the wr.
	 *	2) opcode not reflected from the wr.
	 *	3) read_len not reflected from the wr.
	 *	4) cq_type is RQ_TYPE not SQ_TYPE.
	 */
	if (FUNC15(hw_cqe) && (FUNC3(hw_cqe) == FW_RI_READ_RESP)) {

		/* If we have reached here because of async
		 * event or other error, and have egress error
		 * then drop
		 */
		if (FUNC8(hw_cqe) == 1) {
			if (FUNC6(hw_cqe))
				FUNC28(wq, 0);
			ret = -EAGAIN;
			goto skip_cqe;
		}

		/* If this is an unsolicited read response, then the read
		 * was generated by the kernel driver as part of peer-2-peer
		 * connection setup.  So ignore the completion.
		 */
		if (FUNC13(hw_cqe) == 1) {
			if (FUNC6(hw_cqe))
				FUNC28(wq, 0);
			ret = -EAGAIN;
			goto skip_cqe;
		}

		/*
		 * Eat completions for unsignaled read WRs.
		 */
		if (!wq->sq.oldest_read->signaled) {
			FUNC18(wq);
			ret = -EAGAIN;
			goto skip_cqe;
		}

		/*
		 * Don't write to the HWCQ, so create a new read req CQE
		 * in local memory.
		 */
		FUNC21(wq, hw_cqe, &read_cqe);
		hw_cqe = &read_cqe;
		FUNC18(wq);
	}

	if (FUNC6(hw_cqe) || FUNC31(wq)) {
		*cqe_flushed = (FUNC6(hw_cqe) == T4_ERR_SWFLUSH);
		FUNC28(wq, 0);
	}

	/*
	 * RECV completion.
	 */
	if (FUNC15(hw_cqe)) {

		/*
		 * HW only validates 4 bits of MSN.  So we must validate that
		 * the MSN in the SEND is the next expected MSN.  If its not,
		 * then we complete this with T4_ERR_MSN and mark the wq in
		 * error.
		 */
		if (FUNC32(!FUNC6(hw_cqe) &&
			     FUNC11(hw_cqe) != wq->rq.msn)) {
			FUNC28(wq, 0);
			hw_cqe->header |= FUNC20(FUNC7(T4_ERR_MSN));
		}
		goto proc_cqe;
	}

	/*
	 * If we get here its a send completion.
	 *
	 * Handle out of order completion. These get stuffed
	 * in the SW SQ. Then the SW SQ is walked to move any
	 * now in-order completions into the SW CQ.  This handles
	 * 2 cases:
	 *	1) reaping unsignaled WRs when the first subsequent
	 *	   signaled WR is completed.
	 *	2) out of order read completions.
	 */
	if (!FUNC17(hw_cqe) && (FUNC12(hw_cqe) != wq->sq.cidx)) {
		struct t4_swsqe *swsqe;

		FUNC23("out of order completion going in sw_sq at idx %u\n",
			 FUNC12(hw_cqe));
		swsqe = &wq->sq.sw_sq[FUNC12(hw_cqe)];
		swsqe->cqe = *hw_cqe;
		swsqe->complete = 1;
		ret = -EAGAIN;
		goto flush_wq;
	}

proc_cqe:
	*cqe = *hw_cqe;

	/*
	 * Reap the associated WR(s) that are freed up with this
	 * completion.
	 */
	if (FUNC16(hw_cqe)) {
		int idx = FUNC12(hw_cqe);

		/*
		* Account for any unsignaled completions completed by
		* this signaled completion.  In this case, cidx points
		* to the first unsignaled one, and idx points to the
		* signaled one.  So adjust in_use based on this delta.
		* if this is not completing any unsigned wrs, then the
		* delta will be 0. Handle wrapping also!
		*/
		if (idx < wq->sq.cidx)
			wq->sq.in_use -= wq->sq.size + idx - wq->sq.cidx;
		else
			wq->sq.in_use -= idx - wq->sq.cidx;

		wq->sq.cidx = (uint16_t)idx;
		FUNC23("completing sq idx %u\n", wq->sq.cidx);
		*cookie = wq->sq.sw_sq[wq->sq.cidx].wr_id;
		if (c4iw_wr_log)
			FUNC19(wq, hw_cqe);
		FUNC29(wq);
	} else {
		if (!srq) {
			FUNC23("completing rq idx %u\n", wq->rq.cidx);
			*cookie = wq->rq.sw_rq[wq->rq.cidx].wr_id;
			if (c4iw_wr_log)
				FUNC19(wq, hw_cqe);
			FUNC27(wq);
		} else {
			*cookie = FUNC24(hw_cqe, srq);
		}
		wq->rq.msn++;
		goto skip_cqe;
	}

flush_wq:
	/*
	 * Flush any completed cqes that are now in-order.
	 */
	FUNC22(wq, cq);

skip_cqe:
	if (FUNC17(hw_cqe)) {
		FUNC23("cq %p cqid 0x%x skip sw cqe cidx %u\n",
			 cq, cq->cqid, cq->sw_cidx);
		FUNC30(cq);
	} else {
		FUNC23("cq %p cqid 0x%x skip hw cqe cidx %u\n",
			 cq, cq->cqid, cq->cidx);
		FUNC25(cq);
	}
	return ret;
}