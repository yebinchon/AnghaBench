#define NULL ((void*)0)
typedef unsigned long size_t;  // Customize by platform.
typedef long intptr_t; typedef unsigned long uintptr_t;
typedef long scalar_t__;  // Either arithmetic or pointer type.
/* By default, we understand bool (as a convenience). */
typedef int bool;
#define false 0
#define true 1

/* Forward declarations */

/* Type definitions */
typedef  scalar_t__ tree ;
typedef  enum tree_code { ____Placeholder_tree_code } tree_code ;
typedef  enum built_in_function { ____Placeholder_built_in_function } built_in_function ;
typedef  int /*<<< orphan*/  REAL_VALUE_TYPE ;
typedef  scalar_t__ HOST_WIDE_INT ;

/* Variables and functions */
 scalar_t__ ADDR_EXPR ; 
 scalar_t__ COMPONENT_REF ; 
 scalar_t__ CONVERT_EXPR ; 
 scalar_t__ FUNC0 (scalar_t__) ; 
 int /*<<< orphan*/  FUNC1 (scalar_t__) ; 
 int END_BUILTINS ; 
#define  EQ_EXPR 133 
 scalar_t__ EXACT_DIV_EXPR ; 
 scalar_t__ FUNC2 (scalar_t__) ; 
#define  GE_EXPR 132 
#define  GT_EXPR 131 
 int /*<<< orphan*/  FUNC3 (int /*<<< orphan*/ ) ; 
 int /*<<< orphan*/  FUNC4 (int /*<<< orphan*/ ) ; 
 scalar_t__ INTEGER_CST ; 
 scalar_t__ INTEGER_TYPE ; 
 scalar_t__ FUNC5 (scalar_t__) ; 
#define  LE_EXPR 130 
 int LSHIFT_EXPR ; 
#define  LT_EXPR 129 
 scalar_t__ MAX_EXPR ; 
 scalar_t__ MINUS_EXPR ; 
 scalar_t__ MIN_EXPR ; 
 scalar_t__ NEGATE_EXPR ; 
#define  NE_EXPR 128 
 scalar_t__ NOP_EXPR ; 
 scalar_t__ NULL_TREE ; 
 scalar_t__ PLUS_EXPR ; 
 scalar_t__ FUNC6 (scalar_t__) ; 
 scalar_t__ POSTDECREMENT_EXPR ; 
 scalar_t__ POSTINCREMENT_EXPR ; 
 int PREDECREMENT_EXPR ; 
 int PREINCREMENT_EXPR ; 
 scalar_t__ REAL_CST ; 
 scalar_t__ FUNC7 (int /*<<< orphan*/ ) ; 
 scalar_t__ FUNC8 (int /*<<< orphan*/ ) ; 
 scalar_t__ FUNC9 (int /*<<< orphan*/ ) ; 
 int /*<<< orphan*/  FUNC10 (int /*<<< orphan*/ ) ; 
 int RSHIFT_EXPR ; 
 int /*<<< orphan*/  FUNC11 (scalar_t__) ; 
 scalar_t__ FUNC12 (scalar_t__) ; 
 scalar_t__ FUNC13 (scalar_t__) ; 
 int /*<<< orphan*/  FUNC14 (scalar_t__) ; 
 scalar_t__ FUNC15 (scalar_t__,int) ; 
 int /*<<< orphan*/  FUNC16 (scalar_t__) ; 
 int /*<<< orphan*/  FUNC17 (scalar_t__) ; 
 scalar_t__ FUNC18 (scalar_t__) ; 
 scalar_t__ TRUNC_DIV_EXPR ; 
 scalar_t__ FUNC19 (scalar_t__) ; 
 scalar_t__ FUNC20 (scalar_t__) ; 
 int /*<<< orphan*/  FUNC21 (scalar_t__) ; 
 int /*<<< orphan*/  FUNC22 (scalar_t__) ; 
 scalar_t__ FUNC23 (scalar_t__) ; 
 scalar_t__ FUNC24 (scalar_t__) ; 
 int /*<<< orphan*/  WARN_STRICT_OVERFLOW_COMPARISON ; 
 scalar_t__ FUNC25 (int,scalar_t__,scalar_t__,scalar_t__) ; 
 scalar_t__ FUNC26 (scalar_t__) ; 
 scalar_t__ FUNC27 (scalar_t__,scalar_t__) ; 
 scalar_t__ FUNC28 (scalar_t__,int /*<<< orphan*/ ) ; 
 int FUNC29 (scalar_t__) ; 
 scalar_t__ FUNC30 (scalar_t__,scalar_t__,scalar_t__,int /*<<< orphan*/ ) ; 
 scalar_t__ FUNC31 (int,scalar_t__) ; 
 int /*<<< orphan*/  dconst0 ; 
 scalar_t__ FUNC32 (scalar_t__,scalar_t__,scalar_t__,scalar_t__,scalar_t__) ; 
 scalar_t__ FUNC33 (scalar_t__,scalar_t__*,scalar_t__*) ; 
 int /*<<< orphan*/  flag_errno_math ; 
 scalar_t__ flag_unsafe_math_optimizations ; 
 int /*<<< orphan*/  flag_wrapv ; 
 scalar_t__ FUNC34 (int const,scalar_t__,scalar_t__,scalar_t__) ; 
 scalar_t__ FUNC35 (scalar_t__,scalar_t__) ; 
 scalar_t__ FUNC36 (int,scalar_t__,scalar_t__,scalar_t__) ; 
 scalar_t__ FUNC37 (int,scalar_t__,scalar_t__,scalar_t__) ; 
 scalar_t__ FUNC38 (int,int,scalar_t__,scalar_t__,scalar_t__) ; 
 int /*<<< orphan*/  FUNC39 (char*,int /*<<< orphan*/ ) ; 
 scalar_t__ FUNC40 (int,scalar_t__,scalar_t__,scalar_t__) ; 
 scalar_t__ FUNC41 (int,scalar_t__,scalar_t__,scalar_t__) ; 
 scalar_t__ FUNC42 (int,scalar_t__,scalar_t__,scalar_t__) ; 
 int /*<<< orphan*/  FUNC43 () ; 
 scalar_t__ FUNC44 (int /*<<< orphan*/ ,int) ; 
 scalar_t__ integer_one_node ; 
 int FUNC45 (scalar_t__) ; 
 scalar_t__ integer_zero_node ; 
 int /*<<< orphan*/  FUNC46 (scalar_t__) ; 
 scalar_t__ FUNC47 (scalar_t__,scalar_t__,scalar_t__) ; 
 scalar_t__ FUNC48 (scalar_t__,scalar_t__,int /*<<< orphan*/ ) ; 
 scalar_t__ FUNC49 (int,scalar_t__,scalar_t__,scalar_t__) ; 
 scalar_t__ FUNC50 (scalar_t__) ; 
 scalar_t__ FUNC51 (int /*<<< orphan*/ ) ; 
 int /*<<< orphan*/  size_type_node ; 
 scalar_t__ FUNC52 (scalar_t__) ; 
 int FUNC53 (int) ; 
 scalar_t__ FUNC54 (int /*<<< orphan*/ ,int) ; 
 scalar_t__ FUNC55 (scalar_t__,scalar_t__,int) ; 
 scalar_t__ FUNC56 (scalar_t__,scalar_t__*,scalar_t__*,int*) ; 

__attribute__((used)) static tree
FUNC57 (enum tree_code code, tree type, tree op0, tree op1)
{
  tree arg0, arg1, tem;

  arg0 = op0;
  arg1 = op1;

  FUNC11 (arg0);
  FUNC11 (arg1);

  tem = FUNC40 (code, type, arg0, arg1);
  if (tem != NULL_TREE)
    return tem;

  /* If one arg is a real or integer constant, put it last.  */
  if (FUNC55 (arg0, arg1, true))
    return FUNC34 (FUNC53 (code), type, op1, op0);

  /* Transform comparisons of the form X +- C1 CMP C2 to X CMP C2 +- C1.  */
  if ((FUNC12 (arg0) == PLUS_EXPR || FUNC12 (arg0) == MINUS_EXPR)
      && (FUNC12 (FUNC15 (arg0, 1)) == INTEGER_CST
	  && !FUNC16 (FUNC15 (arg0, 1))
	  && FUNC23 (FUNC18 (arg1)))
      && (FUNC12 (arg1) == INTEGER_CST
	  && !FUNC16 (arg1)))
    {
      tree const1 = FUNC15 (arg0, 1);
      tree const2 = arg1;
      tree variable = FUNC15 (arg0, 0);
      tree lhs;
      int lhs_add;
      lhs_add = FUNC12 (arg0) != PLUS_EXPR;

      lhs = FUNC34 (lhs_add ? PLUS_EXPR : MINUS_EXPR,
			 FUNC18 (arg1), const2, const1);
      if (FUNC12 (lhs) == FUNC12 (arg1)
	  && (FUNC12 (lhs) != INTEGER_CST
	      || !FUNC16 (lhs)))
	{
	  FUNC39 (("assuming signed overflow does not occur "
				  "when changing X +- C1 cmp C2 to "
				  "X cmp C1 +- C2"),
				 WARN_STRICT_OVERFLOW_COMPARISON);
	  return FUNC34 (code, type, variable, lhs);
	}
    }

  /* If this is a comparison of two exprs that look like an ARRAY_REF of the
     same object, then we can fold this to a comparison of the two offsets in
     signed size type.  This is possible because pointer arithmetic is
     restricted to retain within an object and overflow on pointer differences
     is undefined as of 6.5.6/8 and /9 with respect to the signed ptrdiff_t.

     We check flag_wrapv directly because pointers types are unsigned,
     and therefore TYPE_OVERFLOW_WRAPS returns true for them.  That is
     normally what we want to avoid certain odd overflow cases, but
     not here.  */
  if (FUNC6 (FUNC18 (arg0))
      && !flag_wrapv
      && !FUNC22 (FUNC18 (arg0)))
    {
      tree base0, offset0, base1, offset1;

      if (FUNC33 (arg0, &base0, &offset0)
	  && FUNC33 (arg1, &base1, &offset1)
	  && FUNC48 (base0, base1, 0))
        {
	  tree signed_size_type_node;
	  signed_size_type_node = FUNC51 (size_type_node);

	  /* By converting to signed size type we cover middle-end pointer
	     arithmetic which operates on unsigned pointer types of size
	     type size and ARRAY_REF offsets which are properly sign or
	     zero extended from their type in case it is narrower than
	     size type.  */
	  if (offset0 == NULL_TREE)
	    offset0 = FUNC27 (signed_size_type_node, 0);
	  else
	    offset0 = FUNC35 (signed_size_type_node, offset0);
	  if (offset1 == NULL_TREE)
	    offset1 = FUNC27 (signed_size_type_node, 0);
	  else
	    offset1 = FUNC35 (signed_size_type_node, offset1);

	  return FUNC34 (code, type, offset0, offset1);
	}
    }

  if (FUNC2 (FUNC18 (arg0)))
    {
      tree targ0 = FUNC52 (arg0);
      tree targ1 = FUNC52 (arg1);
      tree newtype = FUNC18 (targ0);

      if (FUNC24 (FUNC18 (targ1)) > FUNC24 (newtype))
	newtype = FUNC18 (targ1);

      /* Fold (double)float1 CMP (double)float2 into float1 CMP float2.  */
      if (FUNC24 (newtype) < FUNC24 (FUNC18 (arg0)))
	return FUNC34 (code, type, FUNC35 (newtype, targ0),
			    FUNC35 (newtype, targ1));

      /* (-a) CMP (-b) -> b CMP a  */
      if (FUNC12 (arg0) == NEGATE_EXPR
	  && FUNC12 (arg1) == NEGATE_EXPR)
	return FUNC34 (code, type, FUNC15 (arg1, 0),
			    FUNC15 (arg0, 0));

      if (FUNC12 (arg1) == REAL_CST)
	{
	  REAL_VALUE_TYPE cst;
	  cst = FUNC17 (arg1);

	  /* (-a) CMP CST -> a swap(CMP) (-CST)  */
	  if (FUNC12 (arg0) == NEGATE_EXPR)
	    return FUNC34 (FUNC53 (code), type,
				FUNC15 (arg0, 0),
				FUNC28 (FUNC18 (arg1),
					    FUNC10 (cst)));

	  /* IEEE doesn't distinguish +0 and -0 in comparisons.  */
	  /* a CMP (-0) -> a CMP 0  */
	  if (FUNC9 (cst))
	    return FUNC34 (code, type, arg0,
				FUNC28 (FUNC18 (arg1), dconst0));

	  /* x != NaN is always true, other ops are always false.  */
	  if (FUNC8 (cst)
	      && ! FUNC4 (FUNC21 (FUNC18 (arg1))))
	    {
	      tem = (code == NE_EXPR) ? integer_one_node : integer_zero_node;
	      return FUNC47 (type, tem, arg0);
	    }

	  /* Fold comparisons against infinity.  */
	  if (FUNC7 (cst))
	    {
	      tem = FUNC37 (code, type, arg0, arg1);
	      if (tem != NULL_TREE)
		return tem;
	    }
	}

      /* If this is a comparison of a real constant with a PLUS_EXPR
	 or a MINUS_EXPR of a real constant, we can convert it into a
	 comparison with a revised real constant as long as no overflow
	 occurs when unsafe_math_optimizations are enabled.  */
      if (flag_unsafe_math_optimizations
	  && FUNC12 (arg1) == REAL_CST
	  && (FUNC12 (arg0) == PLUS_EXPR
	      || FUNC12 (arg0) == MINUS_EXPR)
	  && FUNC12 (FUNC15 (arg0, 1)) == REAL_CST
	  && 0 != (tem = FUNC30 (FUNC12 (arg0) == PLUS_EXPR
				      ? MINUS_EXPR : PLUS_EXPR,
				      arg1, FUNC15 (arg0, 1), 0))
	  && ! FUNC14 (tem))
	return FUNC34 (code, type, FUNC15 (arg0, 0), tem);

      /* Likewise, we can simplify a comparison of a real constant with
         a MINUS_EXPR whose first operand is also a real constant, i.e.
         (c1 - x) < c2 becomes x > c1-c2.  */
      if (flag_unsafe_math_optimizations
	  && FUNC12 (arg1) == REAL_CST
	  && FUNC12 (arg0) == MINUS_EXPR
	  && FUNC12 (FUNC15 (arg0, 0)) == REAL_CST
	  && 0 != (tem = FUNC30 (MINUS_EXPR, FUNC15 (arg0, 0),
				      arg1, 0))
	  && ! FUNC14 (tem))
	return FUNC34 (FUNC53 (code), type,
			    FUNC15 (arg0, 1), tem);

      /* Fold comparisons against built-in math functions.  */
      if (FUNC12 (arg1) == REAL_CST
	  && flag_unsafe_math_optimizations
	  && ! flag_errno_math)
	{
	  enum built_in_function fcode = FUNC29 (arg0);

	  if (fcode != END_BUILTINS)
	    {
	      tem = FUNC38 (fcode, code, type, arg0, arg1);
	      if (tem != NULL_TREE)
		return tem;
	    }
	}
    }

  /* Convert foo++ == CONST into ++foo == CONST + INCR.  */
  if (FUNC13 (arg1)
      && (FUNC12 (arg0) == POSTINCREMENT_EXPR
	  || FUNC12 (arg0) == POSTDECREMENT_EXPR)
      /* This optimization is invalid for ordered comparisons
         if CONST+INCR overflows or if foo+incr might overflow.
	 This optimization is invalid for floating point due to rounding.
	 For pointer types we assume overflow doesn't happen.  */
      && (FUNC6 (FUNC18 (arg0))
	  || (FUNC5 (FUNC18 (arg0))
	      && (code == EQ_EXPR || code == NE_EXPR))))
    {
      tree varop, newconst;

      if (FUNC12 (arg0) == POSTINCREMENT_EXPR)
	{
	  newconst = FUNC34 (PLUS_EXPR, FUNC18 (arg0),
				  arg1, FUNC15 (arg0, 1));
	  varop = FUNC25 (PREINCREMENT_EXPR, FUNC18 (arg0),
			  FUNC15 (arg0, 0),
			  FUNC15 (arg0, 1));
	}
      else
	{
	  newconst = FUNC34 (MINUS_EXPR, FUNC18 (arg0),
				  arg1, FUNC15 (arg0, 1));
	  varop = FUNC25 (PREDECREMENT_EXPR, FUNC18 (arg0),
			  FUNC15 (arg0, 0),
			  FUNC15 (arg0, 1));
	}


      /* If VAROP is a reference to a bitfield, we must mask
	 the constant by the width of the field.  */
      if (FUNC12 (FUNC15 (varop, 0)) == COMPONENT_REF
	  && FUNC0 (FUNC15 (FUNC15 (varop, 0), 1))
	  && FUNC44 (FUNC1 (FUNC15
					 (FUNC15 (varop, 0), 1)), 1))
	{
	  tree fielddecl = FUNC15 (FUNC15 (varop, 0), 1);
	  HOST_WIDE_INT size = FUNC54 (FUNC1 (fielddecl), 1);
	  tree folded_compare, shift;

	  /* First check whether the comparison would come out
	     always the same.  If we don't do that we would
	     change the meaning with the masking.  */
	  folded_compare = FUNC34 (code, type,
					FUNC15 (varop, 0), arg1);
	  if (FUNC12 (folded_compare) == INTEGER_CST)
	    return FUNC47 (type, folded_compare, varop);

	  shift = FUNC27 (NULL_TREE,
				 FUNC24 (FUNC18 (varop)) - size);
	  shift = FUNC35 (FUNC18 (varop), shift);
	  newconst = FUNC34 (LSHIFT_EXPR, FUNC18 (varop),
				  newconst, shift);
	  newconst = FUNC34 (RSHIFT_EXPR, FUNC18 (varop),
				  newconst, shift);
	}

      return FUNC34 (code, type, varop, newconst);
    }

  if (FUNC12 (FUNC18 (arg0)) == INTEGER_TYPE
      && (FUNC12 (arg0) == NOP_EXPR
	  || FUNC12 (arg0) == CONVERT_EXPR))
    {
      /* If we are widening one operand of an integer comparison,
	 see if the other operand is similarly being widened.  Perhaps we
	 can do the comparison in the narrower type.  */
      tem = FUNC42 (code, type, arg0, arg1);
      if (tem)
	return tem;

      /* Or if we are changing signedness.  */
      tem = FUNC41 (code, type, arg0, arg1);
      if (tem)
	return tem;
    }

  /* If this is comparing a constant with a MIN_EXPR or a MAX_EXPR of a
     constant, we can simplify it.  */
  if (FUNC12 (arg1) == INTEGER_CST
      && (FUNC12 (arg0) == MIN_EXPR
	  || FUNC12 (arg0) == MAX_EXPR)
      && FUNC12 (FUNC15 (arg0, 1)) == INTEGER_CST)
    {
      tem = FUNC49 (code, type, op0, op1);
      if (tem)
	return tem;
    }

  /* Simplify comparison of something with itself.  (For IEEE
     floating-point, we can only do some of these simplifications.)  */
  if (FUNC48 (arg0, arg1, 0))
    {
      switch (code)
	{
	case EQ_EXPR:
	  if (! FUNC2 (FUNC18 (arg0))
	      || ! FUNC3 (FUNC21 (FUNC18 (arg0))))
	    return FUNC31 (1, type);
	  break;

	case GE_EXPR:
	case LE_EXPR:
	  if (! FUNC2 (FUNC18 (arg0))
	      || ! FUNC3 (FUNC21 (FUNC18 (arg0))))
	    return FUNC31 (1, type);
	  return FUNC34 (EQ_EXPR, type, arg0, arg1);

	case NE_EXPR:
	  /* For NE, we can only do this simplification if integer
	     or we don't honor IEEE floating point NaNs.  */
	  if (FUNC2 (FUNC18 (arg0))
	      && FUNC3 (FUNC21 (FUNC18 (arg0))))
	    break;
	  /* ... fall through ...  */
	case GT_EXPR:
	case LT_EXPR:
	  return FUNC31 (0, type);
	default:
	  FUNC43 ();
	}
    }

  /* If we are comparing an expression that just has comparisons
     of two integer values, arithmetic expressions of those comparisons,
     and constants, we can simplify it.  There are only three cases
     to check: the two values can either be equal, the first can be
     greater, or the second can be greater.  Fold the expression for
     those three values.  Since each value must be 0 or 1, we have
     eight possibilities, each of which corresponds to the constant 0
     or 1 or one of the six possible comparisons.

     This handles common cases like (a > b) == 0 but also handles
     expressions like  ((x > y) - (y > x)) > 0, which supposedly
     occur in macroized code.  */

  if (FUNC12 (arg1) == INTEGER_CST && FUNC12 (arg0) != INTEGER_CST)
    {
      tree cval1 = 0, cval2 = 0;
      int save_p = 0;

      if (FUNC56 (arg0, &cval1, &cval2, &save_p)
	  /* Don't handle degenerate cases here; they should already
	     have been handled anyway.  */
	  && cval1 != 0 && cval2 != 0
	  && ! (FUNC13 (cval1) && FUNC13 (cval2))
	  && FUNC18 (cval1) == FUNC18 (cval2)
	  && FUNC5 (FUNC18 (cval1))
	  && FUNC19 (FUNC18 (cval1))
	  && FUNC19 (FUNC18 (cval2))
	  && ! FUNC48 (FUNC20 (FUNC18 (cval1)),
				FUNC19 (FUNC18 (cval2)), 0))
	{
	  tree maxval = FUNC19 (FUNC18 (cval1));
	  tree minval = FUNC20 (FUNC18 (cval1));

	  /* We can't just pass T to eval_subst in case cval1 or cval2
	     was the same as ARG1.  */

	  tree high_result
		= FUNC34 (code, type,
			       FUNC32 (arg0, cval1, maxval,
					   cval2, minval),
			       arg1);
	  tree equal_result
		= FUNC34 (code, type,
			       FUNC32 (arg0, cval1, maxval,
					   cval2, maxval),
			       arg1);
	  tree low_result
		= FUNC34 (code, type,
			       FUNC32 (arg0, cval1, minval,
					   cval2, maxval),
			       arg1);

	  /* All three of these results should be 0 or 1.  Confirm they are.
	     Then use those values to select the proper code to use.  */

	  if (FUNC12 (high_result) == INTEGER_CST
	      && FUNC12 (equal_result) == INTEGER_CST
	      && FUNC12 (low_result) == INTEGER_CST)
	    {
	      /* Make a 3-bit mask with the high-order bit being the
		 value for `>', the next for '=', and the low for '<'.  */
	      switch ((FUNC45 (high_result) * 4)
		      + (FUNC45 (equal_result) * 2)
		      + FUNC45 (low_result))
		{
		case 0:
		  /* Always false.  */
		  return FUNC47 (type, integer_zero_node, arg0);
		case 1:
		  code = LT_EXPR;
		  break;
		case 2:
		  code = EQ_EXPR;
		  break;
		case 3:
		  code = LE_EXPR;
		  break;
		case 4:
		  code = GT_EXPR;
		  break;
		case 5:
		  code = NE_EXPR;
		  break;
		case 6:
		  code = GE_EXPR;
		  break;
		case 7:
		  /* Always true.  */
		  return FUNC47 (type, integer_one_node, arg0);
		}

	      if (save_p)
		return FUNC50 (FUNC25 (code, type, cval1, cval2));
	      return FUNC34 (code, type, cval1, cval2);
	    }
	}
    }

  /* Fold a comparison of the address of COMPONENT_REFs with the same
     type and component to a comparison of the address of the base
     object.  In short, &x->a OP &y->a to x OP y and
     &x->a OP &y.a to x OP &y  */
  if (FUNC12 (arg0) == ADDR_EXPR
      && FUNC12 (FUNC15 (arg0, 0)) == COMPONENT_REF
      && FUNC12 (arg1) == ADDR_EXPR
      && FUNC12 (FUNC15 (arg1, 0)) == COMPONENT_REF)
    {
      tree cref0 = FUNC15 (arg0, 0);
      tree cref1 = FUNC15 (arg1, 0);
      if (FUNC15 (cref0, 1) == FUNC15 (cref1, 1))
	{
	  tree op0 = FUNC15 (cref0, 0);
	  tree op1 = FUNC15 (cref1, 0);
	  return FUNC34 (code, type,
			      FUNC26 (op0),
			      FUNC26 (op1));
	}
    }

  /* We can fold X/C1 op C2 where C1 and C2 are integer constants
     into a single range test.  */
  if ((FUNC12 (arg0) == TRUNC_DIV_EXPR
       || FUNC12 (arg0) == EXACT_DIV_EXPR)
      && FUNC12 (arg1) == INTEGER_CST
      && FUNC12 (FUNC15 (arg0, 1)) == INTEGER_CST
      && !FUNC46 (FUNC15 (arg0, 1))
      && !FUNC16 (FUNC15 (arg0, 1))
      && !FUNC16 (arg1))
    {
      tem = FUNC36 (code, type, arg0, arg1);
      if (tem != NULL_TREE)
	return tem;
    }

  return NULL_TREE;
}